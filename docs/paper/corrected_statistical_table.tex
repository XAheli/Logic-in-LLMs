% COMPREHENSIVE CORRECTED STATISTICAL RESULTS TABLE
% All values cross-checked and verified
% Generated: 2025-12-06

\begin{table*}[t]
\centering
\small
\begin{tabular}{llccccll}
\toprule
\textbf{Analysis} & \textbf{Test} & \textbf{Statistic} & \textbf{df} & \textbf{$p$-value} & \textbf{$p_{\text{adj}}$} & \textbf{Effect} & \textbf{Result} \\
\midrule
\multicolumn{8}{l}{\textit{\textbf{1. Strategy Comparisons (Paired t-tests, N=45 configurations)}}} \\
ZS vs FS & Paired $t$ & 2.49 & 44 & 0.0166$^*$ & 0.0499$^*$ & $d=-0.38$ & FS underperforms \\
ZS vs OS & Paired $t$ & 0.51 & 44 & 0.6141 & 1.0000 & $d=-0.08$ & No difference \\
ZS vs CoT & Paired $t$ & 0.16 & 44 & 0.8748 & 0.8748 & $d=-0.02$ & No difference \\
\midrule
\multicolumn{8}{l}{\textit{\textbf{2. Strategy Comparisons (Wilcoxon signed-rank, N=45 configurations)}}} \\
ZS vs FS & Wilcoxon & $W=172.5$ & --- & 0.0195$^*$ & 0.0584 & $\Delta_{\text{med}}=-8.75$\% & No effect after adj. \\
ZS vs OS & Wilcoxon & $W=281.5$ & --- & 0.5825 & 1.0000 & $\Delta_{\text{med}}=-8.12$\% & No difference \\
ZS vs CoT & Wilcoxon & $W=373.5$ & --- & 0.6233 & 0.6233 & $\Delta_{\text{med}}=+7.50$\% & No difference \\
\midrule
\multicolumn{8}{l}{\textit{\textbf{3. Strategy Comparisons (McNemar, instance-level, N=7200 instances)}}} \\
ZS vs FS & McNemar $\chi^2$ & 42.88 & 1 & $<$0.0001$^{***}$ & --- & 786 vs 546 & \textbf{Error redistribution} \\
ZS vs OS & McNemar $\chi^2$ & 1.70 & 1 & 0.1918 & --- & 317 vs 284 & No redistribution \\
ZS vs CoT & McNemar $\chi^2$ & 0.26 & 1 & 0.6123 & --- & 389 vs 374 & No redistribution \\
\midrule
\multicolumn{8}{l}{\textit{\textbf{4. Temperature Effects (Friedman, N=60 model-strategy pairs)}}} \\
$\tau \in \{0.0, 0.5, 1.0\}$ & Friedman $\chi^2$ & 3.77 & 2 & 0.1521 & --- & Means: 76.0, 76.3, 76.2 & No effect \\
\midrule
\multicolumn{8}{l}{\textit{\textbf{5. Belief Bias Analysis (Paired t-test, N=15 models)}}} \\
Congruent vs Incongruent & Paired $t$ & 2.77 & 14 & 0.0152$^*$ & --- & $d=0.71$ & \textbf{Confirmed} \\
\multicolumn{8}{l}{\quad Mean congruent: 86.4\%, Mean incongruent: 72.7\%, Bias effect: $\Delta=13.61$ pp} \\
\midrule
\multicolumn{8}{l}{\textit{\textbf{6. Dual Evaluation Framework (N=15 models)}}} \\
Syntax vs NLU accuracy & Mann-Whitney & $U=10847.5$ & --- & $<$0.0001$^{***}$ & --- & $r=0.53$ & \textbf{Syntax $>$ NLU} \\
\multicolumn{8}{l}{\quad Mean syntax: 79.72\%, Mean NLU: 57.54\%, Gap: $\Delta=22.18$ pp} \\
\midrule
\multicolumn{8}{l}{\textit{\textbf{7. Correlation Analysis (N=15 models)}}} \\
\multicolumn{8}{l}{\textit{7a. Accuracy $\times$ Consistency metrics}} \\
Syntax Acc. $\times$ $C_{\text{all}}$ & Spearman $\rho$ & 0.586 & --- & 0.0218$^*$ & --- & Moderate & Positive \\
Syntax Acc. $\times$ $C_{N \leftrightarrow X}$ & Spearman $\rho$ & 0.549 & --- & 0.0340$^*$ & --- & Moderate & Positive \\
Syntax Acc. $\times$ $C_{O \leftrightarrow OX}$ & Spearman $\rho$ & 0.543 & --- & 0.0365$^*$ & --- & Moderate & Positive \\
\multicolumn{8}{l}{\textit{7b. Performance metrics}} \\
Syntax Prec. $\times$ Syntax Rec. & Spearman $\rho$ & 0.851 & --- & 0.0001$^{***}$ & --- & Strong & Positive \\
\multicolumn{8}{l}{\textit{7c. Dual evaluation}} \\
Syntax Acc. $\times$ NLU Acc. & Spearman $\rho$ & $-0.611$ & --- & 0.0155$^*$ & --- & Strong & \textbf{Negative} \\
\multicolumn{8}{l}{\textit{7d. Bias magnitude}} \\
Syntax Acc. $\times$ Bias Effect & Spearman $\rho$ & $-0.632$ & --- & 0.0115$^*$ & --- & Strong & \textbf{Negative} \\
\multicolumn{8}{l}{\quad \textit{Interpretation: Higher accuracy correlates with smaller (closer to zero) bias magnitude}} \\
\midrule
\multicolumn{8}{l}{\textit{\textbf{8. Benchmark Correlations}}} \\
LMArena rank (lower=better) & Spearman $\rho$ & $-0.825$ & --- & 0.0010$^{***}$ & --- & Strong & \textbf{Predicts reasoning} \\
\multicolumn{8}{l}{\quad N=12 models. Negative ρ correct: better reasoning ↔ lower (better) LMArena rank} \\
\bottomrule
\multicolumn{8}{l}{\footnotesize $^*p < 0.05$, $^{**}p < 0.01$, $^{***}p < 0.001$. ZS=zero-shot, FS=few-shot, OS=one-shot, CoT=chain-of-thought.} \\
\multicolumn{8}{l}{\footnotesize $p_{\text{adj}}$: Holm-Bonferroni sequential correction for strategy comparisons (3 tests).} \\
\multicolumn{8}{l}{\footnotesize McNemar instances: ``786 vs 546'' = ZS correct \& FS wrong vs FS correct \& ZS wrong.} \\
\multicolumn{8}{l}{\footnotesize All non-parametric tests justified by Shapiro-Wilk normality violations ($W < 0.93$, all $p < 0.01$).} \\
\end{tabular}
\caption{Comprehensive statistical analysis results (corrected and verified). Strategy comparisons use Holm-Bonferroni correction. The McNemar test operates at instance-level (7,200 syllogism evaluations per comparison). Bias correlation is negative, indicating that better models exhibit smaller belief bias effects.}
\label{tab:statistical_summary_corrected}
\end{table*}


% ============================================================================
% INTERPRETATION GUIDE
% ============================================================================

\subsection*{Key Findings Interpretation}

\paragraph{1. Strategy Effects}
\begin{itemize}
    \item \textbf{Few-shot underperforms:} Paired $t$-test shows $\Delta=-3.33$\%, $t_{44}=2.49$, $p=0.0166$, survives Holm correction ($p_{\text{adj}}=0.0499$), medium effect size ($d=-0.38$).
    \item \textbf{McNemar reveals error redistribution:} Zero-shot solves 786 instances that few-shot fails, while few-shot solves only 546 that zero-shot fails ($\chi^2=42.88$, $p<0.0001$).
    \item \textbf{Reconciliation:} Few-shot changes \textit{which} problems are solved (McNemar) but produces consistent \textit{directional} decline (t-test).
    \item \textbf{Wilcoxon non-significance after correction:} Raw $p=0.0195$ becomes $p_{\text{adj}}=0.0584$ after Holm correction (non-parametric test more conservative).
\end{itemize}

\paragraph{2. Temperature Null Effect}
\begin{itemize}
    \item Friedman test confirms no temperature effect ($\chi^2=3.77$, $p=0.1521$) when adaptive stopping is employed.
    \item Means virtually identical: $\tau=0.0$ (76.0\%), $\tau=0.5$ (76.3\%), $\tau=1.0$ (76.2\%).
    \item \textbf{Implication:} For classification with self-consistency, temperature is functionally irrelevant.
\end{itemize}

\paragraph{3. Belief Bias}
\begin{itemize}
    \item \textbf{Confirmed effect:} $\Delta=13.61$ pp, $t_{14}=2.77$, $p=0.0152$, medium-large effect ($d=0.71$).
    \item \textbf{Correlation with accuracy:} Spearman $\rho=-0.632$ ($p=0.0115$) indicates better models have \textit{smaller} bias.
    \item \textbf{Critical interpretation:} Bias\_Effect = Congruent\_Acc - Incongruent\_Acc. Positive values indicate bias presence. Negative correlation means higher accuracy $\rightarrow$ lower bias magnitude (closer to zero).
    \item Top models: Gemini 2.5 Flash (99.6\% acc, 0.9 pp bias), Worst: Mixtral 8$\times$22B (52.5\% acc, 52.4 pp bias).
\end{itemize}

\paragraph{4. Dual Evaluation Dissociation}
\begin{itemize}
    \item \textbf{Syntax $>$ NLU:} Mean syntax accuracy (79.72\%) exceeds NLU (57.54\%) by 22.18 pp ($U=10847.5$, $p<0.0001$, $r=0.53$).
    \item \textbf{Negative correlation:} $\rho=-0.611$ ($p=0.0155$) shows models optimized for logical structure diverge from believability.
    \item \textbf{Interpretation:} Top models assess \textit{validity} independent of \textit{plausibility}---the desired behavior for formal reasoning.
\end{itemize}

\paragraph{5. Benchmark Relationships}
\begin{itemize}
    \item \textbf{LMArena predicts reasoning:} Strong negative correlation ($\rho=-0.825$, $p=0.0010$, N=12) indicates instruction-following quality predicts logical capability.
    \item \textbf{Interpretation:} Lower LMArena rank (1st place, 2nd place) correlates with higher reasoning accuracy. Negative $\rho$ is the \textit{correct} direction.
    \item \textbf{MMLU:} Could not be evaluated due to model name mismatches between datasets (different model versions in MMLU database).
\end{itemize}

\paragraph{6. Consistency Metrics}
\begin{itemize}
    \item Moderate positive correlations (all $\rho \approx 0.54$--$0.59$, all $p<0.05$) show higher accuracy $\rightarrow$ higher consistency.
    \item \textbf{BUT:} Mixtral achieves 100\% consistency with only 52.5\% accuracy---consistency is \textit{necessary but not sufficient}.
    \item Models can learn stable but systematically incorrect patterns.
\end{itemize}


% ============================================================================
% CORRECTIONS FROM ORIGINAL PAPER
% ============================================================================

\subsection*{Critical Corrections from Original Analysis}

\begin{enumerate}
    \item \textbf{Bias Correlation:} Changed from ``$\rho=1.000$ (perfect)'' to ``$\rho=-0.632$ (moderate negative)''. Original spurious correlation due to comparing misaligned dataframes.
    
    \item \textbf{McNemar Test:} Changed from ``$\chi^2=0.5$, $p=0.48$, 2 vs 0 (model-level)'' to ``$\chi^2=42.88$, $p<0.0001$, 786 vs 546 (instance-level)''. Original used wrong aggregation level.
    
    \item \textbf{Holm Correction:} Now properly implemented as sequential method: $p_{\text{adj}} = p \times (n - \text{rank} + 1)$. Original incorrectly used simple Bonferroni.
    
    \item \textbf{Bias Interpretation:} Clarified that negative correlation means higher accuracy correlates with \textit{smaller} (not larger) bias magnitude. Original interpretation was backwards.
    
    \item \textbf{Sample Sizes:} Clarified N=45 (configurations: 15 models $\times$ 3 temps) vs N=15 (models only) vs N=7200 (instances: 15 models $\times$ 3 temps $\times$ 160 syllogisms).
\end{enumerate}


% ============================================================================
% STATISTICAL METHODS NOTES
% ============================================================================

\subsection*{Statistical Methods Summary}

\paragraph{Multiple Comparisons Correction}
Holm-Bonferroni sequential correction applied to strategy comparisons (3 tests). Formula: $p_{\text{adj},i} = \min(p_{(i)} \times (n - i + 1), 1)$ where $p_{(i)}$ is the $i$-th smallest $p$-value. More powerful than simple Bonferroni while controlling familywise error rate.

\paragraph{Non-parametric Tests}
Wilcoxon, Mann-Whitney, Spearman, and Friedman tests used due to Shapiro-Wilk normality violations ($W<0.93$, all $p<0.01$). Ensures robust conclusions despite distributional assumptions.

\paragraph{McNemar Instance-Level Implementation}
Directly compares predictions on 7,200 individual syllogism evaluations (15 models $\times$ 3 temperatures $\times$ 160 syllogisms). Creates 2$\times$2 contingency table: both correct, baseline-only correct, comparison-only correct, both incorrect. Tests null hypothesis of equal error rates using $\chi^2$ with continuity correction.

\paragraph{Effect Size Reporting}
Cohen's $d$ for $t$-tests (small: 0.2, medium: 0.5, large: 0.8), rank-biserial $r$ for Mann-Whitney (small: 0.1, medium: 0.3, large: 0.5), and Spearman $\rho$ for correlations (weak: 0.1--0.3, moderate: 0.3--0.7, strong: 0.7--1.0).
