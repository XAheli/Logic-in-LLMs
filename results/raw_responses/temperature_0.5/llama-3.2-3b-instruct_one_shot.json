{
  "metadata": {
    "model": "llama-3.2-3b-instruct",
    "strategy": "one_shot",
    "temperature": 0.5,
    "timestamp": "2025-12-02T05:49:36.109050"
  },
  "results": [
    {
      "syllogism_id": "SYL_001",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:25:57.500201",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_001",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:26:05.086267",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_001",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:26:12.192134",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_001",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:26:20.077962",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_002",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:26:27.450475",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_002",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:26:34.850541",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_002",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:26:42.503645",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_002",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:26:49.774473",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_003",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:26:57.352274",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_003",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:27:05.039234",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_003",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:27:12.404985",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_003",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:27:19.880406",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_004",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:27:27.459766",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_004",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:27:34.677553",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_004",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.8,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 2,
      "incorrect_count": 8,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:27:49.686537",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_004",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:27:57.400950",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_005",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:28:05.044474",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_005",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:28:12.617919",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_005",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:28:19.852895",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_005",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:28:27.596241",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_006",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:28:34.709333",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_006",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:28:42.411448",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_006",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:28:49.996061",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_006",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:28:57.191874",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_007",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:29:04.925033",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_007",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:29:12.728272",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_007",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:29:19.914765",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_007",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:29:27.194995",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_008",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:29:35.080425",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_008",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 0.6,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 4,
      "incorrect_count": 6,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:29:50.003534",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_008",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:29:57.562755",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_008",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:30:05.033381",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_009",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:30:12.186766",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_009",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 0.6,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 4,
      "incorrect_count": 6,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:30:27.676531",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_009",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:30:34.957773",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_009",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 0.6,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 6,
      "incorrect_count": 4,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:30:49.724532",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 10,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_010",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:30:57.435246",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_010",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 0.7,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 7,
      "incorrect_count": 3,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:31:12.217402",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 10,
          "response": "correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_010",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:31:20.118047",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_010",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.6,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 4,
      "incorrect_count": 6,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:31:34.818522",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_011",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:31:42.242981",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_011",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 0.9,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 9,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:31:57.218233",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "Correct.",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 10,
          "response": "correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_011",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:32:04.765225",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_011",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.6,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 4,
      "incorrect_count": 6,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:32:19.821402",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_012",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:32:27.225668",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_012",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 0.7,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 3,
      "incorrect_count": 7,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:32:42.458489",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_012",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.9,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 1,
      "incorrect_count": 9,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:32:57.242004",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_012",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 5,
      "incorrect_count": 0,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:33:04.712842",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_013",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:33:12.558782",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_013",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:33:19.738166",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_013",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:33:27.447739",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_013",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.8,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 2,
      "incorrect_count": 8,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:33:42.451669",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_014",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:33:49.758601",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_014",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:33:57.410522",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_014",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:34:04.953585",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_014",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 0.8,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 8,
      "incorrect_count": 2,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:34:19.846432",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 10,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_015",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:34:27.505189",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_015",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 0.6,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 4,
      "incorrect_count": 6,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:34:42.251155",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_015",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:34:49.906573",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_015",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 5,
      "incorrect_count": 0,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:34:57.249457",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_016",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:35:05.007691",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_016",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:35:12.294865",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_016",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:35:20.171721",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_016",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 0.8,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 8,
      "incorrect_count": 2,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:35:34.903813",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "Correct.",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 10,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_017",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:35:42.431726",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_017",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:35:50.154383",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_017",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:35:57.634888",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_017",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 5,
      "incorrect_count": 0,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:36:04.890190",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_018",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:36:12.788132",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_018",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:36:19.767559",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_018",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.9,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 1,
      "incorrect_count": 9,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:36:35.212122",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_018",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 0.9,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 9,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:36:49.766126",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 10,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_019",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:36:57.683454",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect. \n\nThe conclusion does not logically follow from the premises. The premises state that no well-educated people are impartial, but this does not imply that some judges are not well-educated people. It only implies that some judges may be well-educated, as they are not necessarily well-educated people.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_019",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:37:04.795793",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_019",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:37:12.486690",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_019",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 0.9,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 9,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:37:27.262800",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 10,
          "response": "correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_020",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:37:35.220505",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_020",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:37:42.261337",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_020",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:37:50.274267",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_020",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.8,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 2,
      "incorrect_count": 8,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:38:05.147059",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_021",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:38:12.718599",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_021",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:38:20.217766",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_021",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:38:27.520530",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_021",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.5,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 5,
      "incorrect_count": 5,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:38:42.805620",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 7,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_022",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:38:49.900455",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_022",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:38:57.516621",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_022",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:39:05.003988",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_022",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.6,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 4,
      "incorrect_count": 6,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:39:19.994212",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_023",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:39:27.269963",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_023",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:39:35.339313",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_023",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:39:42.514368",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_023",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:39:50.289973",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_024",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:39:57.545508",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_024",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:40:05.071145",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect. \n\nThe syllogism is incorrect because if some vitamin tablets are expensive and no nutritional things are expensive, it does not necessarily mean that some vitamin tablets are not nutritional things. The premises do not provide enough information to conclude that vitamin tablets are not nutritional things.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_024",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:40:12.613008",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_024",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.7,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 3,
      "incorrect_count": 7,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:40:27.711758",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_025",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:40:35.339344",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_025",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 0.9,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 1,
      "incorrect_count": 9,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:40:50.253747",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_025",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.8,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 2,
      "incorrect_count": 8,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:41:05.199424",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_025",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.7,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 3,
      "incorrect_count": 7,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:41:19.983026",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_026",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:41:28.493431",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_026",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:41:36.161224",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_026",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:41:43.330895",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_026",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:41:51.269480",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_027",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:41:58.804916",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_027",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:42:06.039213",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_027",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.8,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 2,
      "incorrect_count": 8,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:42:21.059541",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_027",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.8,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 2,
      "incorrect_count": 8,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:42:36.056964",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 8,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_028",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:42:43.785623",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_028",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:42:51.295745",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_028",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:42:58.712377",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_028",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:43:06.212522",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_029",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:43:13.581446",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_029",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:43:21.239506",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect. \n\nThe conclusion does not necessarily follow from the premises. The premises only provide information about the relationship between being a police dog and being vicious, and being highly trained. They do not provide information about the relationship between being highly trained and not being a police dog.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_029",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:43:28.612796",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_029",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:43:36.059832",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_030",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:43:43.692374",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_030",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:43:50.865802",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_030",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:43:58.617208",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_030",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.9,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 1,
      "incorrect_count": 9,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:44:13.584874",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_031",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:44:21.093219",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_031",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 0.9,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 10,
      "correct_count": 1,
      "incorrect_count": 9,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:44:36.198404",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_031",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:44:43.602370",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_031",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.9,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 1,
      "incorrect_count": 9,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:44:58.823474",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect. \n\nThe conclusion does not follow logically from the premises because the premises do not provide any information about the relationship between speckles and swimbles. They only provide information about brightle things and swimbles, which is irrelevant to the conclusion.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_032",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:45:06.008296",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_032",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:45:13.577559",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_032",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:45:21.293347",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_032",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:45:28.832448",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_033",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:45:36.297840",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_033",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:45:43.433967",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_033",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:45:51.668526",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_033",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 0.6,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 10,
      "correct_count": 4,
      "incorrect_count": 6,
      "stopped_early": false,
      "timestamp": "2025-12-02T05:46:06.215098",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "correct.",
          "vote": "correct"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 6,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 7,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 8,
          "response": "correct",
          "vote": "correct"
        },
        {
          "iteration": 9,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 10,
          "response": "correct",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_034",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:46:13.798261",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_034",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:46:21.091941",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_034",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:46:28.635658",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_034",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:46:36.111756",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_035",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:46:43.398664",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_035",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:46:51.121115",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_035",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:46:58.534518",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_035",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:47:06.051818",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_036",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:47:13.620076",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_036",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:47:20.910146",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_036",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:47:28.644332",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_036",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:47:36.116909",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_037",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:47:43.570817",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_037",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:47:51.214136",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_037",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:47:58.750668",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_037",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:48:05.907834",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_038",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:48:13.575685",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_038",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:48:21.043139",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_038",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:48:28.446424",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_038",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:48:36.111786",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_039",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:48:43.648946",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_039",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:48:51.169588",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_039",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:48:58.451126",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_039",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:49:06.139775",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_040",
      "variant": "N",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:49:13.448180",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_040",
      "variant": "O",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:49:21.184048",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_040",
      "variant": "X",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:49:28.454920",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_040",
      "variant": "OX",
      "model_key": "llama-3.2-3b-instruct",
      "temperature": 0.5,
      "prompting_strategy": "one_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 5,
      "correct_count": 0,
      "incorrect_count": 5,
      "stopped_early": true,
      "timestamp": "2025-12-02T05:49:36.108811",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 2,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 3,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 4,
          "response": "Incorrect.",
          "vote": "incorrect"
        },
        {
          "iteration": 5,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    }
  ]
}