{
  "metadata": {
    "model": "llama-3.2-1b-instruct",
    "strategy": "zero_shot",
    "temperature": 0.0,
    "timestamp": "2025-12-02T10:01:44.812103"
  },
  "results": [
    {
      "syllogism_id": "SYL_001",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:46.107893",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_001",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:47.599278",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_001",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:49.300513",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_001",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:50.722632",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_002",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:52.118320",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_002",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:53.652515",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_002",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:55.164905",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_002",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:56.651562",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_003",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:58.323460",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_003",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:57:59.639812",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_003",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:01.197682",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_003",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:02.600645",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_004",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:04.139225",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_004",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:05.764875",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_004",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:07.279541",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_004",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:08.650436",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_005",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:10.136651",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_005",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:11.714971",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_005",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:13.251427",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_005",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:14.654623",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_006",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:16.147952",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_006",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:17.629933",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_006",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:19.294363",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_006",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:20.727152",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_007",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:22.170064",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_007",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:23.804369",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_007",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:25.158438",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_007",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:26.696744",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_008",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:28.192555",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_008",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:29.753084",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_008",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:31.162295",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_008",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:32.605174",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_009",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:34.059584",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_009",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:35.676974",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_009",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:37.307461",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_009",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:38.637122",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_010",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:40.183153",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_010",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:41.719953",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_010",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:43.129743",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_010",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:44.688142",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_011",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:46.148859",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_011",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:47.658614",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_011",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:49.194462",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_011",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:50.837770",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_012",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:52.367617",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_012",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:53.700560",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_012",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:55.172163",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_012",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:56.672335",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_013",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:58.756314",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_013",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:58:59.775399",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_013",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:01.174588",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_013",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:02.770804",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_014",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:04.350404",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_014",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:05.664629",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_014",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:07.217394",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_014",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:08.630849",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_015",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:10.187766",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_015",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:11.752662",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_015",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:13.360355",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_015",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:14.806552",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_016",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:16.123637",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_016",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:17.663688",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_016",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:19.264580",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_016",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:20.638708",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_017",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:22.148818",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_017",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:23.654592",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_017",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:25.202906",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_017",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:26.595950",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_018",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:28.208983",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_018",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:29.777944",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_018",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:31.122573",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_018",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:32.713978",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_019",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:34.256816",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_019",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:35.649187",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_019",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:37.220095",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_019",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:38.793046",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_020",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:40.394095",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_020",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:41.923448",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_020",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:43.187284",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_020",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:44.804129",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_021",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:46.159331",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_021",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:47.767123",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_021",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:49.090465",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_021",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:50.647853",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_022",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:52.271217",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_022",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:53.834921",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_022",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:55.280576",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_022",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:56.759773",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_023",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:58.213313",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_023",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T09:59:59.850904",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_023",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:01.319715",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_023",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:02.679093",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_024",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:04.254416",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_024",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:05.656006",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_024",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:07.170919",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_024",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "valid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:08.788399",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_025",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:10.230217",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_025",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:11.729668",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_025",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:13.371653",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_025",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:14.699261",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_026",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:16.235202",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_026",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:17.771280",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_026",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:19.272840",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_026",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:20.945352",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_027",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:22.179624",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_027",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:23.608739",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_027",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:25.279719",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_027",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:26.780898",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_028",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:28.216366",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_028",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:29.855233",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_028",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:31.167798",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_028",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:32.721184",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_029",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:34.171864",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_029",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:35.654230",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_029",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:37.285124",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_029",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:39.070233",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_030",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:40.197980",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_030",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:41.733414",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_030",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:43.076506",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_030",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:44.652862",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_031",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:46.238167",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_031",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:47.876569",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_031",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:49.311239",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_031",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:50.873113",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_032",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:52.285930",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_032",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "believable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:53.886718",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_032",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:55.162492",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_032",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:56.667789",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_033",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:58.322709",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_033",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "correct",
      "confidence": 1.0,
      "is_correct_syntax": false,
      "is_correct_NLU": false,
      "total_iterations": 1,
      "correct_count": 1,
      "incorrect_count": 0,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:00:59.652846",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Correct.",
          "vote": "correct"
        }
      ]
    },
    {
      "syllogism_id": "SYL_033",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:01.291607",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_033",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:02.797099",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_034",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:04.261829",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_034",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:05.675131",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_034",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:07.302529",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_034",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:08.765803",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_035",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:10.154176",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_035",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:11.737205",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_035",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:13.127451",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_035",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:14.790087",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_036",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:16.242285",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_036",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:17.675730",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_036",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:19.110388",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_036",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:20.746568",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_037",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:22.284700",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_037",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:23.718168",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_037",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:25.308806",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_037",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:26.699698",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_038",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:28.172873",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_038",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:29.699556",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_038",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:31.142789",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_038",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:32.626025",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_039",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:34.170640",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_039",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:35.753659",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_039",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:37.183915",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_039",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:38.744411",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_040",
      "variant": "N",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:40.203896",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_040",
      "variant": "O",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:41.740704",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_040",
      "variant": "X",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:43.172637",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    },
    {
      "syllogism_id": "SYL_040",
      "variant": "OX",
      "model_key": "llama-3.2-1b-instruct",
      "temperature": 0.0,
      "prompting_strategy": "zero_shot",
      "ground_truth_syntax": "invalid",
      "ground_truth_NLU": "unbelievable",
      "predicted": "incorrect",
      "confidence": 1.0,
      "is_correct_syntax": true,
      "is_correct_NLU": true,
      "total_iterations": 1,
      "correct_count": 0,
      "incorrect_count": 1,
      "stopped_early": false,
      "timestamp": "2025-12-02T10:01:44.811854",
      "raw_responses": [
        {
          "iteration": 1,
          "response": "Incorrect.",
          "vote": "incorrect"
        }
      ]
    }
  ]
}